{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5726cfa-667e-4523-9435-762f84fdadbf",
   "metadata": {},
   "source": [
    "# Capturing Data Change of MySQL from Binlog and passing to Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b078414-9690-4981-9fde-93fe1e03f09a",
   "metadata": {},
   "source": [
    "### importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "328ae396-76c2-41b7-b068-b08b8517c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "from pymysqlreplication import BinLogStreamReader\n",
    "from pymysqlreplication.row_event import DeleteRowsEvent,UpdateRowsEvent,WriteRowsEvent\n",
    "from pymysqlreplication.event import QueryEvent,RotateEvent\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.admin import AdminClient,NewTopic\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import subprocess\n",
    "import socket\n",
    "import threading\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from typing import Tuple, Optional,List\n",
    "from datetime import datetime,date,timedelta,time as dt_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee142461-53ad-4fe4-9518-b7c311c1e62f",
   "metadata": {},
   "source": [
    "### configurations used in our code\n",
    "- change it as per your requirements, like bootstrap.servers,zookeeper_cnf,mysql_cnf and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e6cb68b-9a5d-4c58-92d3-a1aaa043f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka configuration for 3-node cluster\n",
    "kafka_cnf = {\n",
    "    'bootstrap.servers': 'localhost:9093,localhost:9094,localhost:9095',  # Kafka broker addresses\n",
    "    'acks': 'all',                                    # Wait for all in-sync replicas to acknowledge the record, ensuring durability\n",
    "    'enable.idempotence': True,                       # Ensure that messages are produced exactly once, avoiding duplicates\n",
    "    'retries': 3,                                     # Increase the number of retries to handle transient failures\n",
    "    'retry.backoff.ms': 50,                           # Decrease backoff time for retries to reduce latency on transient failures\n",
    "    'delivery.timeout.ms': 2000,                      # Maximum time to wait for a message delivery, including retries\n",
    "    'request.timeout.ms': 1000,                       # Reduce request timeout to quickly fail fast on unresponsive brokers\n",
    "    'max.in.flight.requests.per.connection': 5,       # Allows multiple requests to be in-flight to maximize throughput\n",
    "    'batch.size': 65536,                              # Increase batch size to accumulate more messages before sending, reducing overhead\n",
    "    'linger.ms': 1,                                   # Time to wait before sending a batch; helps to fill the batch size before sending\n",
    "    # 'buffer.memory': 67108864,                      # Increase buffer memory to allow more data to be buffered in memory before sending\n",
    "    'compression.type': 'gzip',                       # Compression type to reduce the size of data sent over the network; can also use 'snappy' for faster performance\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Zookeeper configuration for 3-node cluster\n",
    "zookeeper_cnf = [\n",
    "    {'host': 'localhost', 'port': 2181},          # zookeeper servers ip:port , you can refer your zookeeper.properties file\n",
    "    {'host': 'localhost', 'port': 2182},\n",
    "    {'host': 'localhost', 'port': 2183}\n",
    "]\n",
    "\n",
    "# mysql_configuration\n",
    "mysql_cnf={\n",
    "    'host':'localhost',       # mysql server ip, port number, user name and user password \n",
    "    'port':3308,\n",
    "    'user':'root',\n",
    "    'passwd':'Root@123'\n",
    "}\n",
    "\n",
    "\n",
    "# mysql_server id, change it to yours\n",
    "mysql_server_id=2\n",
    "\n",
    "# binlog starting file from which we starts to read binlog, you can change as per your requirements\n",
    "binlog_starting_file='binlog.000001'  \n",
    "\n",
    "# log file path\n",
    "log_file_path='/media/susan/F4707AD3707A9BD4/MysqlCdc_Python_kafka/3_node_kafka_and_3_node_zookeeper/node1/cdc.log'\n",
    "# Checkpointing\n",
    "checkpoint_file = '/media/susan/F4707AD3707A9BD4/MysqlCdc_Python_kafka/3_node_kafka_and_3_node_zookeeper/node1/checkpoint.json'\n",
    "\n",
    "# file path of the file which is required for checking status of kafka \n",
    "kafka_broker_api_versions='/media/susan/F4707AD3707A9BD4/MysqlCdc_Python_kafka/single_node_kafka_and_zookeeper/bin/kafka-broker-api-versions'\n",
    "\n",
    "# for email alerting, change it to yours\n",
    "sender_email = os.getenv('EMAIL_USER')    # sender_email address,receiver_email address and password of sender_email address \n",
    "receiver_email = os.getenv('EMAIL_USER')  # I just give from environment variable and for password use app passport for email (recommended)\n",
    "password = os.getenv('EMAIL_PASSWORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d897a3a2-b12c-4b46-beea-3d00ffea886f",
   "metadata": {},
   "source": [
    "### initializing logging configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321f71d8-85fc-4a89-8a87-31d03ae28490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up custom logger\n",
    "logger = logging.getLogger('my_logger')     # you can give any name as per your requirement\n",
    "logger.setLevel(logging.INFO)               # Setting the logging level, here highest level is info and only that level messages are logged \n",
    "\n",
    "# setting custom File handler that log messages to a file\n",
    "file_handler = logging.FileHandler(log_file_path)\n",
    "file_handler.setLevel(logging.INFO)         # Log level for file handler\n",
    "\n",
    "# setting custom Formatter to define the log message format\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')  # it log in the format [time_of_logging  level_of_log  actual_messages]\n",
    "\n",
    "# adding that custom formatter to file handler\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# setting propagate to False to prevent messages from being sent to the root logger, cause sometimes root logger prints the log messages in console so\n",
    "logger.propagate = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349c4eb-a088-43e5-8469-332eb8a03aab",
   "metadata": {},
   "source": [
    "### this function is responsible for reading binlog from mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd2ec683-2707-4338-8b1c-5057a972fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_binlog(log_file: str, log_pos: int, retries: int = 3, backoff: int = 3):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to read a MySQL binlog starting from a specific file and position, with retry logic.\n",
    "    \n",
    "    Args:\n",
    "    - log_file (str): The name of the binlog file to read from.\n",
    "    - log_pos (int): The position in the binlog file to start reading from.\n",
    "    - retries (int): Number of retry attempts in case of failure (default: 3).\n",
    "    - backoff (int): Time (in seconds) to wait between retries (default: 5 seconds).\n",
    "\n",
    "    Returns:\n",
    "    - BinLogStreamReader object if successful, otherwise logs error after retries are exhausted.\n",
    "    \"\"\"\n",
    "    \n",
    "    attempt=0\n",
    "    while attempt<retries:\n",
    "        try:\n",
    "            # initializing a BinLogStreamReader to read binlog events\n",
    "            stream=BinLogStreamReader(                                  \n",
    "                connection_settings=mysql_cnf,\n",
    "                server_id=mysql_server_id,\n",
    "                blocking=True,\n",
    "                resume_stream=True,\n",
    "                only_events=[WriteRowsEvent, UpdateRowsEvent, DeleteRowsEvent, QueryEvent, RotateEvent],\n",
    "                log_file=log_file,\n",
    "                log_pos=log_pos\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Successfully read binlog from file '{log_file}' at position '{log_pos}'.\")\n",
    "            return stream\n",
    "        except Exception as e:\n",
    "            attempt+=1\n",
    "\n",
    "            if attempt<retries:\n",
    "                logger.info(f\"Retrying to read binlog from '{log_file}' at position '{log_pos}'... (Attempt {attempt}/{retries})\")\n",
    "                time.sleep(backoff)\n",
    "            else:\n",
    "                logger.error(f\"Max retries reached. Binlog reading from '{log_file}' at position '{log_pos}' failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b38e8-efbb-44cb-be24-9e973fee5d44",
   "metadata": {},
   "source": [
    "### following two functions are responsible for checkpoint mechanism\n",
    "- load_checkpoint() is responsible for loading last saved binlog file and position\n",
    "- save_checkpoint() is responsible for saving last processed binlog file and position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ded94c0-7718-4fe6-81dd-66a9623e92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint() -> Tuple[Optional[str], Optional[int]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Function to load the last saved binlog position (checkpoint) from a file (checkpoint_file).\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple (log_file, log_pos) if a valid checkpoint is found.\n",
    "    - (None, None) if the checkpoint file is missing, empty, or corrupted.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the checkpoint file exists and is not empty\n",
    "    if os.path.exists(checkpoint_file) and os.path.getsize(checkpoint_file) > 0:\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            try:\n",
    "                checkpoint = json.load(f)\n",
    "                return checkpoint['log_file'], checkpoint['log_pos']\n",
    "            except json.JSONDecodeError:\n",
    "                # Handle invalid JSON or empty file\n",
    "                logger.error(\"Checkpoint file is corrupted. Starting from the beginning.\")\n",
    "                return None, None\n",
    "    else:\n",
    "        # No checkpoint file or the file is empty\n",
    "        logger.info(\"No valid checkpoint found. Starting from the beginning.\")\n",
    "        return None, None\n",
    "        \n",
    "\n",
    "def save_checkpoint(log_file: str, log_pos: int) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function to save the current binlog file and position to a checkpoint file.\n",
    "    \n",
    "    Args:\n",
    "    - log_file (str): The current binlog file name.\n",
    "    - log_pos (int): The current position in the binlog file.\n",
    "    \"\"\"\n",
    "\n",
    "    # creating a dictionary to store the log file and position\n",
    "    checkpoint = {\n",
    "        'log_file': log_file,\n",
    "        'log_pos': log_pos\n",
    "    }\n",
    "\n",
    "    # writing the checkpoint data to the checkpoint file in JSON format\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(checkpoint, f)   # serializing the dictionary into json format and writing into file\n",
    "        logger.info(f\"Checkpoint saved: log_file='{log_file}', log_pos={log_pos}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf8340-0d24-46d5-8c5f-c6d3cc2af4a3",
   "metadata": {},
   "source": [
    "### following functions:\n",
    "- sanitize_topic_name() is responisble for making appropriate topic name\n",
    "- create_topic_if_not_exists() is responsible for making kafka topics dynamically/automatically\n",
    "-  datetime_to_str() is responsible for converting data related data into string cause json serializaiton doesn't support date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9882126a-93a0-4641-99d6-96e8be823aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_topic_name(topic_name: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Function to sanitize Kafka topic names by replacing invalid characters.\n",
    "\n",
    "    Args:\n",
    "    - topic_name (str): The original topic name.\n",
    "\n",
    "    Returns:\n",
    "    - sanitized_name (str): The sanitized topic name with a max length of 255 characters.\n",
    "    \"\"\"\n",
    "\n",
    "    # replacing all characters that are not allowed in Kafka topic names (anything except letters, digits, '_', '-', or '.') with underscores.\n",
    "    sanitized_name = re.sub(r'[^a-zA-Z0-9._-]', '_',topic_name)\n",
    "    return sanitized_name[:255]\n",
    "\n",
    "\n",
    "\n",
    "def create_topic_if_not_exists(database: str, table: str) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function to create a Kafka topic if it does not already exist.\n",
    "    \n",
    "    Args:\n",
    "    - database (str): The name of the database.\n",
    "    - table (str): The name of the table.\n",
    "    \n",
    "    The function first sanitizes the topic name, checks if it exists, \n",
    "    and if not, creates a new topic with one partition and a replication factor of 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    topic_name=f\"{database}.{table}\"\n",
    "    topic_name = sanitize_topic_name(topic_name)  # ensuring topic names are sanitized\n",
    "    admin_client = AdminClient(kafka_cnf)         # initializing AdminClinet to manage kafka topics\n",
    "\n",
    "    existing_topics=admin_client.list_topics(timeout=10).topics.keys()   # retriveing the list of existing kafka topics\n",
    "\n",
    "    # checking the topic is already exists or not\n",
    "    if topic_name not in existing_topics:\n",
    "        try:\n",
    "            # creating a new kafka topic\n",
    "            new_topic=admin_client.create_topics(\n",
    "                [NewTopic(topic_name,num_partitions=1,replication_factor=2)]\n",
    "            )\n",
    "            # ensuring the creation of the topic is successful by checking the result\n",
    "            new_topic[topic_name].result()\n",
    "            logger.info(f\"Topic '{topic_name}' is created.\")\n",
    "           \n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating topic '{topic_name}': {e}\")\n",
    "          \n",
    "    else:\n",
    "        logger.info(f\"Topic '{topic_name}' already exists.\")\n",
    "     \n",
    "\n",
    "\n",
    "def datetime_to_str(dt):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to convert various datetime-related objects to a string format.\n",
    "\n",
    "    Args:\n",
    "    - dt: The datetime-related object to convert.\n",
    "\n",
    "    Returns:\n",
    "    - A string representation of the datetime, date, time, or timedelta object.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(dt, datetime):        # Full datetime with date and time\n",
    "        return dt.isoformat()           # returns ISO 8601 string format\n",
    "    elif isinstance(dt, date):          # Date without time\n",
    "        return dt.isoformat()\n",
    "    elif isinstance(dt, dt_time):       # Time only (hours, minutes, seconds)\n",
    "        return dt.strftime('%H:%M:%S')  # Format time as string\n",
    "    elif isinstance(dt, timedelta):     # Handle timedelta\n",
    "        total_seconds = int(dt.total_seconds())\n",
    "        hours, remainder = divmod(total_seconds, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        return f\"{hours:02}:{minutes:02}:{seconds:02}\"  # Formatting as HH:MM:SS\n",
    "    else:\n",
    "        return dt  # Return unchanged if not a datetime, date, time, or timedelta object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83df6897-b249-4316-a53e-f69a99cf2fa7",
   "metadata": {},
   "source": [
    "### following functions:\n",
    "- initialize_producer() is responsible for initializing kafka producer\n",
    "- produce_message() is responsible for producing messages to kafka topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9baf02e7-3f80-4983-8b11-009511f79822",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = None  # initializing the producer variable globally\n",
    "\n",
    "def initialize_producer(retries: int = 5, base_backoff: int = 2) -> Producer:\n",
    "\n",
    "    \"\"\"\n",
    "    Initialize Kafka Producer with exponential backoff retry mechanism.\n",
    "\n",
    "    Args:\n",
    "        retries: Number of times to retry initialization if it fails.\n",
    "        base_backoff: Base time in seconds for the exponential backoff between retries.\n",
    "\n",
    "    Returns:\n",
    "        Producer: An instance of the initialized Kafka producer.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If max retries are reached and producer initialization fails.\n",
    "    \"\"\"\n",
    "    \n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            producer = Producer(**kafka_cnf)  # initializing kafka producer\n",
    "            logger.info(\"Kafka producer initialized successfully.\")\n",
    "            # print('kafka producer initialized successfully')\n",
    "            return producer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize producer: {e}\")\n",
    "            attempt += 1\n",
    "            if attempt < retries:\n",
    "                backoff_time = base_backoff * (2 ** (attempt - 1))  # applying exponential backoff\n",
    "                logger.info(f\"Retrying to initialize producer in {backoff_time} seconds... ({attempt}/{retries})\")\n",
    "                time.sleep(backoff_time)\n",
    "            else:\n",
    "                logger.critical(\"Max retries reached for producer initialization.\")\n",
    "                send_email_alert('Failed Producer initialization Alert','Failed to initialize producer and max retires reached to initialization')\n",
    "                raise\n",
    "\n",
    "\n",
    "def produce_message(topic: Optional[str], message: Optional[str], retries: int = 3, backoff: int = 1) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Produce a message to a specified Kafka topic with retry mechanism.\n",
    "\n",
    "    Args:\n",
    "        topic: The Kafka topic where the message will be sent.\n",
    "        message: The message content to be produced.\n",
    "        retries: Number of retries in case of failure to produce the message.\n",
    "        backoff: Base backoff time in seconds before each retry.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    global producer  # Ensuring to use the global producer\n",
    "    if producer is None:\n",
    "        logger.error(\"Producer not initialized. Cannot produce message.\")\n",
    "        return  # Exit if producer is not initialized\n",
    "    \n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            # producing messages to the kafka topic\n",
    "            producer.produce(topic, value=message)  \n",
    "            producer.flush()  # ensuring the message is sent\n",
    "            logger.info(f\"Message: '{message}' is successfully produced to topic '{topic}'.\")\n",
    "            # print(f\"Message successfully produced to topic '{topic}'.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to produce message to topic '{topic}':\\n Message content: '{message} \\n error:{e}'\")\n",
    "            # Check if the error is related to PID acquisition\n",
    "            if \"Failed to acquire idempotence PID\" in str(e):\n",
    "                logger.info(\"Encountered PID acquisition issue; retrying...\")\n",
    "                attempt += 1\n",
    "                time.sleep(backoff)  # Wait before retrying\n",
    "            else:\n",
    "                attempt += 1  # Increment for other types of errors\n",
    "                if attempt < retries:\n",
    "                    logger.info(f\"Retrying to produce message to topic '{topic}'... Attempt {attempt} of {retries}.\")\n",
    "                    time.sleep(backoff)\n",
    "                else:\n",
    "                    logger.critical(\"Max retries reached. Message production failed.\")\n",
    "                    send_email_alert('Message Production Failure Alert', \n",
    "                            f\"Failed to produce message to topic '{topic}'.\\n\"\n",
    "                            f\"Message Content: '{message}'.\\n\"\n",
    "                            f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f87e5-8cc1-496c-a797-f8f849339434",
   "metadata": {},
   "source": [
    "### following function is responsible for extracting database and table name, we need it for extracting columns name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea32bb01-c5c7-4c5c-ad54-e649550612c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_and_schema(query):\n",
    "\n",
    "    \"\"\"\n",
    "    Extract the schema and table names from a given SQL query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The SQL query string from which to extract schema and table names.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the schema name and table name.\n",
    "               If not found, it returns ('unknown_schema', 'unknown_table').\n",
    "    \"\"\"\n",
    "    \n",
    "    # regex pattern to capture schema and table names from various SQL commands\n",
    "    match = re.search(\n",
    "        r\"(ALTER TABLE|CREATE TABLE|DROP TABLE|INSERT INTO|UPDATE|DELETE FROM)\\s+\"\n",
    "        r\"(?:`?(\\w+)`?\\.)?`?(\\w+)`?\", query, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        # capture the schema name (if provided) or set to 'unknown_schema'\n",
    "        schema = match.group(2) if match.group(2) else 'unknown_schema'  \n",
    "        # capture the table name\n",
    "        table = match.group(3)\n",
    "        return schema, table\n",
    "\n",
    "    # If match isn't found, return default values\n",
    "    return 'unknown_schema', 'unknown_table'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9bd833-22f0-430c-960c-b1ff4b18947c",
   "metadata": {},
   "source": [
    "### following function is responsible for extracting columns name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48682d90-ee0b-4cb9-87a0-a1e4d30cf6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names_from_schema(schema: str, table: str) -> List[str]:\n",
    "\n",
    "    \"\"\"\n",
    "    Retrieve column names from a specified table in the given schema.\n",
    "\n",
    "    Args:\n",
    "        schema (str): The database schema (database name).\n",
    "        table (str): The table name from which to retrieve column names.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of column names from the specified table, or an empty list if an error occurs.\n",
    "    \"\"\"\n",
    "\n",
    "    # creating a copy of mysql_cnf to avoid modifying the global variable\n",
    "    local_mysql_cnf = copy.deepcopy(mysql_cnf)\n",
    "    local_mysql_cnf['database'] = schema  # adding database \n",
    "\n",
    "    try:\n",
    "        # connectioning mysql server\n",
    "        with pymysql.connect(**local_mysql_cnf) as connection:\n",
    "            with connection.cursor() as cursor:\n",
    "                query = \"\"\"\n",
    "                    SELECT COLUMN_NAME \n",
    "                    FROM INFORMATION_SCHEMA.COLUMNS \n",
    "                    WHERE TABLE_SCHEMA = %s AND TABLE_NAME = %s\n",
    "                    ORDER BY ORDINAL_POSITION  -- Ensures the columns are in the defined order\n",
    "                \"\"\"\n",
    "                # retrieving the columns name\n",
    "                cursor.execute(query, (schema, table))\n",
    "                column_names = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "                return column_names\n",
    "\n",
    "    except pymysql.MySQLError as e:\n",
    "        logger.error(f\"Error retrieving column names from {schema}.{table}: {e}\")\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3258b3-3b25-4266-9bcb-82d85aab5e33",
   "metadata": {},
   "source": [
    "### following functions;\n",
    "- send_email_alert() responsible for sending alert emails\n",
    "- check_mysql_health() responsible for status checking of mysql server; it is running or not\n",
    "- check_kafka_health() is responsible for status checking of kafka server\n",
    "- check_zookeeper_health() is responsible for status chechking of zookeeper server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5e21bff-3215-413a-92b7-a5a609b7b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def send_email_alert(subject, message):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to send an email alert.\n",
    "\n",
    "    Args:\n",
    "        subject (str): The subject line of the email.\n",
    "        message (str): The content of the email message.\n",
    "    \n",
    "    This function retrieves sender and receiver email addresses and the password from \n",
    "    environment variables for security. It creates an email message and uses an SMTP \n",
    "    server to send the email alert.\n",
    "    \"\"\"\n",
    "    \n",
    "    from_email=sender_email\n",
    "    to_email=receiver_email \n",
    "    passwd = password\n",
    "    \n",
    "    msg = MIMEText(message)    # email message\n",
    "    msg['Subject'] = subject   # email subject\n",
    "    msg['From'] = from_email\n",
    "    msg['To'] = to_email\n",
    "    \n",
    "    try:\n",
    "        # sending alert email\n",
    "        with smtplib.SMTP('smtp.gmail.com', 587) as server:  # Update with your SMTP server\n",
    "            server.starttls()\n",
    "            server.login(from_email, passwd)\n",
    "            server.sendmail(from_email, to_email, msg.as_string())\n",
    "            logger.info(\"Alert email sent successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to send email: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "def check_mysql_health(mysql_cnf, retries=5, delay=10):\n",
    "    \"\"\"\n",
    "    Function to check MySQL server health with retry mechanism.\n",
    "\n",
    "    Args:\n",
    "        mysql_cnf (dict): A dictionary containing MySQL connection parameters (user, password, host, port).\n",
    "        retries (int): Number of retry attempts if MySQL server is down.\n",
    "        delay (int): Delay (in seconds) between retry attempts.\n",
    "    \n",
    "    This function attempts to ping the MySQL server using the provided configuration and\n",
    "    retries if the server is down. Returns True if the server is healthy, otherwise False\n",
    "    after all retries.\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            # Executing the mysqladmin command to ping the MySQL server\n",
    "            result = subprocess.run(\n",
    "                [\n",
    "                    'mysqladmin',               # Command to interact with MySQL server\n",
    "                    '-u', mysql_cnf['user'], \n",
    "                    '-p' + mysql_cnf['passwd'], \n",
    "                    '-h', mysql_cnf['host'], \n",
    "                    '-P', str(mysql_cnf['port']),\n",
    "                    'ping'                     # Command to check if the server is alive\n",
    "                ],\n",
    "                stdout=subprocess.PIPE, stderr=subprocess.PIPE   # Capture standard output and error\n",
    "            )\n",
    "\n",
    "            # Checking if the command executed successfully\n",
    "            if result.returncode == 0:\n",
    "                logger.info(f\"MySQL server {mysql_cnf['host']}:{mysql_cnf['port']} is healthy.\")\n",
    "                return True  # Server is healthy\n",
    "            else:\n",
    "                # Raise an exception if the command fails\n",
    "                raise Exception(result.stderr.decode())\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Attempt {attempt}: MySQL check failed for server {mysql_cnf['host']}:{mysql_cnf['port']}: {str(e)}\")\n",
    "            \n",
    "            # If the number of retries is reached, send an alert\n",
    "            if attempt == retries:\n",
    "                error_message = f\"MySQL server {mysql_cnf['host']}:{mysql_cnf['port']} is down after {retries} attempts.\"\n",
    "                subject = \"MySQL Health Alert\"\n",
    "                send_email_alert(subject, error_message)  # Send email alert\n",
    "                logger.critical(f\"MySQL server {mysql_cnf['host']}:{mysql_cnf['port']} is down after {retries} retries.\")\n",
    "                return False\n",
    "            else:\n",
    "                logger.warning(f\"MySQL server {mysql_cnf['host']}:{mysql_cnf['port']} is still down. Retrying in {delay} seconds... (Attempt {attempt+1}/{retries})\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "\n",
    "\n",
    "def check_kafka_cluster_health(kafka_cnf, retries=5, delay=10):\n",
    "    \"\"\"\n",
    "    Function to check the health of each Kafka broker in a cluster.\n",
    "    \n",
    "    Args:\n",
    "        kafka_cnf (dict): Kafka configuration with 'bootstrap.servers'.\n",
    "        retries (int): Number of retry attempts if Kafka broker is down.\n",
    "        delay (int): Delay (in seconds) between retry attempts.\n",
    "    \"\"\"\n",
    "    kafka_servers = kafka_cnf['bootstrap.servers'].split(',')  # List of Kafka brokers\n",
    "\n",
    "    kafka_status = []  # To store health status of each broker\n",
    "    \n",
    "    for Kserver in kafka_servers:\n",
    "        for attempt in range(1, retries + 1):\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    ['kafka-broker-api-versions', '--bootstrap-server', Kserver],\n",
    "                    stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "                )\n",
    "                if result.returncode == 0:\n",
    "                    logger.info(f\"Kafka broker {Kserver} is healthy.\")\n",
    "                    kafka_status.append(True)  # Broker is healthy\n",
    "                    break\n",
    "                else:\n",
    "                    raise Exception(result.stderr.decode())\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Attempt {attempt}: Kafka check failed for broker {Kserver}: {str(e)}\")\n",
    "                \n",
    "                if attempt == retries:  # If last retry also fails\n",
    "                    send_email_alert(f\"Kafka Health Alert\", f\"Kafka broker {Kserver} is down after {retries} attempts.\")\n",
    "                    logger.critical(f\"Kafka broker {Kserver} is down after {retries} retries.\")\n",
    "                    kafka_status.append(False)  # Broker is unhealthy\n",
    "                else:\n",
    "                    logger.warning(f\"Retrying Kafka broker {Kserver} in {delay} seconds... (Attempt {attempt+1}/{retries})\")\n",
    "                    time.sleep(delay)\n",
    "\n",
    "    # Check if any broker is healthy\n",
    "    if any(kafka_status):\n",
    "        return True  # At least one broker is healthy\n",
    "    else:\n",
    "        return False  # All brokers are down\n",
    "\n",
    "                    \n",
    "\n",
    "def check_zookeeper_cluster_health(zookeeper_cnf, retries=5, delay=10):\n",
    "    \"\"\"\n",
    "    Function to check the health of each Zookeeper node in the cluster.\n",
    "\n",
    "    Args:\n",
    "        zookeeper_cnf (list): List of dictionaries containing Zookeeper connection parameters (host, port).\n",
    "        retries (int): Number of retry attempts if Zookeeper server is down.\n",
    "        delay (int): Delay (in seconds) between retry attempts.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if at least one Zookeeper node is healthy, False if all are down.\n",
    "    \"\"\"\n",
    "    zk_status = []  # To track the health of each Zookeeper node\n",
    "    \n",
    "    for node in zookeeper_cnf:\n",
    "        zk_host = node['host']\n",
    "        zk_port = node['port']\n",
    "        \n",
    "        for attempt in range(1, retries + 1):\n",
    "            try:\n",
    "                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                    s.settimeout(5)\n",
    "                    s.connect((zk_host, zk_port))\n",
    "                    s.sendall(b'ruok\\n')  # Send the 'ruok' (Are you okay?) command\n",
    "                    response = s.recv(1024).decode().strip()  # Read the response\n",
    "\n",
    "                if response.lower() == 'imok':\n",
    "                    logger.info(f\"Zookeeper server {zk_host}:{zk_port} is healthy.\")\n",
    "                    zk_status.append(True)  # Mark this node as healthy\n",
    "                    break  # Exit retry loop for this node\n",
    "                else:\n",
    "                    raise Exception(f\"Unexpected Zookeeper response: {response}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Attempt {attempt}: Zookeeper check failed for server {zk_host}:{zk_port}: {str(e)}\")\n",
    "                \n",
    "                if attempt == retries:\n",
    "                    send_email_alert(\n",
    "                        \"Zookeeper Health Alert\",\n",
    "                        f\"Zookeeper server {zk_host}:{zk_port} is down after {retries} attempts.\"\n",
    "                    )\n",
    "                    logger.critical(f\"Zookeeper server {zk_host}:{zk_port} is down after {retries} retries.\")\n",
    "                    zk_status.append(False)  # Mark this node as unhealthy\n",
    "                else:\n",
    "                    logger.warning(f\"Retrying Zookeeper server {zk_host}:{zk_port} in {delay} seconds... (Attempt {attempt+1}/{retries})\")\n",
    "                    time.sleep(delay)\n",
    "\n",
    "    # Check if any Zookeeper node is healthy\n",
    "    if any(zk_status):\n",
    "        return True  # At least one node is healthy\n",
    "    else:\n",
    "        return False  # All nodes are down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e72df9f-4822-42ac-9d89-9a88f4488da1",
   "metadata": {},
   "source": [
    "### following function monitor_system_continuously() is responsible for checkig status of the servers; mysql, kafka, zookeeper at every hour \n",
    "- if the system is down for 3 iteration then the checking gonna stop, so for continuos 3 iteration if the servers are down then each alert email gonna trigger for each hour (i.e. after 3rd alert the checking will be stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5d86490-3012-422c-961c-abf6fcd39b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def monitor_system_continuously(interval: int = 3600):\n",
    "\n",
    "    \"\"\"\n",
    "    Continuously monitor the system at specified intervals and send alerts if services are down.\n",
    "\n",
    "    Args:\n",
    "        interval (int): The time (in seconds) to wait between checks. Default is 3600 seconds (1 hour).\n",
    "    \n",
    "    This function performs health checks for MySQL, Kafka, and Zookeeper services up to three times. \n",
    "    If any service is found to be down, it sends alerts and logs the event. The monitoring runs in a \n",
    "    separate thread.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_checks = 3\n",
    "    total_checks = 0\n",
    "\n",
    "    while total_checks < max_checks:\n",
    "        try:\n",
    "            # checking the health of MySQL, Kafka, and Zookeeper\n",
    "            mysql_health = check_mysql_health(mysql_cnf)\n",
    "            kafka_health = check_kafka_cluster_health(kafka_cnf)\n",
    "            zookeeper_health = check_zookeeper_cluster_health(zookeeper_cnf)\n",
    "            \n",
    "            if not (mysql_health and kafka_health and zookeeper_health):\n",
    "                raise Exception(\"Mysql server or all 3 Zookeepers servers or all 3 Kafka servers are down.\")\n",
    "\n",
    "            total_checks=0      # Reset successful checks counter after a successful check\n",
    "            logger.info(\"System's health check successful.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"System monitoring failed: {e}\")\n",
    "\n",
    "        total_checks += 1\n",
    "        time.sleep(interval)  # Wait for the specified interval (10 minutes) before checking again\n",
    "\n",
    "    logger.info(\"Completed three checks for each hour.\")\n",
    "\n",
    "# Start the monitoring thread when the program starts\n",
    "monitor_thread = threading.Thread(target=monitor_system_continuously, args=(3600,))  # Checks every 20 minutes\n",
    "monitor_thread.daemon = True  # Ensures the thread will exit when the main program exits\n",
    "monitor_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368d232-d801-4d7a-b8cd-167fd4dbe177",
   "metadata": {},
   "source": [
    "### Main function of program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bfd0011-4b8e-4dc8-9cbb-1666ec9434f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%3|1727697933.801|FAIL|rdkafka#producer-5| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT)\n",
      "%3|1727697934.045|FAIL|rdkafka#producer-4| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 21 identical error(s) suppressed)\n",
      "%3|1727697934.301|FAIL|rdkafka#producer-5| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT)\n",
      "%3|1727697934.802|FAIL|rdkafka#producer-5| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT)\n",
      "%3|1727697935.302|FAIL|rdkafka#producer-5| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 1 identical error(s) suppressed)\n",
      "%3|1727697935.802|FAIL|rdkafka#producer-5| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 1 identical error(s) suppressed)\n",
      "%3|1727697936.802|FAIL|rdkafka#producer-5| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 1 identical error(s) suppressed)\n",
      "%3|1727697940.823|FAIL|rdkafka#producer-6| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT)\n",
      "%3|1727697941.323|FAIL|rdkafka#producer-6| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT)\n",
      "%3|1727697941.823|FAIL|rdkafka#producer-6| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT)\n",
      "%3|1727697942.323|FAIL|rdkafka#producer-6| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 1 identical error(s) suppressed)\n",
      "%3|1727697943.323|FAIL|rdkafka#producer-6| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 1 identical error(s) suppressed)\n",
      "%3|1727697944.049|FAIL|rdkafka#producer-4| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 24 identical error(s) suppressed)\n",
      "%3|1727697945.824|FAIL|rdkafka#producer-6| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 1 identical error(s) suppressed)\n",
      "%3|1727697964.057|FAIL|rdkafka#producer-4| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 30 identical error(s) suppressed)\n",
      "%3|1727697964.555|FAIL|rdkafka#producer-4| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 13 identical error(s) suppressed)\n",
      "%3|1727697965.310|FAIL|rdkafka#producer-5| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 24 identical error(s) suppressed)\n",
      "%3|1727697968.312|FAIL|rdkafka#producer-5| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 25 identical error(s) suppressed)\n",
      "%3|1727697968.813|FAIL|rdkafka#producer-5| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 13 identical error(s) suppressed)\n",
      "%3|1727697973.832|FAIL|rdkafka#producer-6| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 26 identical error(s) suppressed)\n",
      "%3|1727697974.060|FAIL|rdkafka#producer-4| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 18 identical error(s) suppressed)\n",
      "%3|1727697974.332|FAIL|rdkafka#producer-6| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 13 identical error(s) suppressed)\n",
      "%3|1727697975.832|FAIL|rdkafka#producer-6| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 25 identical error(s) suppressed)\n",
      "%3|1727697994.568|FAIL|rdkafka#producer-4| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 25 identical error(s) suppressed)\n",
      "%3|1727697995.820|FAIL|rdkafka#producer-5| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 19 identical error(s) suppressed)\n",
      "%3|1727697998.570|FAIL|rdkafka#producer-4| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 18 identical error(s) suppressed)\n",
      "%3|1727697999.321|FAIL|rdkafka#producer-5| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 21 identical error(s) suppressed)\n",
      "%3|1727698000.321|FAIL|rdkafka#producer-5| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 22 identical error(s) suppressed)\n",
      "%3|1727698004.840|FAIL|rdkafka#producer-6| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 22 identical error(s) suppressed)\n",
      "%3|1727698005.341|FAIL|rdkafka#producer-6| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 21 identical error(s) suppressed)\n",
      "%3|1727698006.341|FAIL|rdkafka#producer-6| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 18 identical error(s) suppressed)\n",
      "%3|1727698006.574|FAIL|rdkafka#producer-4| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 19 identical error(s) suppressed)\n",
      "%3|1727698026.084|FAIL|rdkafka#producer-4| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 25 identical error(s) suppressed)\n",
      "%3|1727698028.583|FAIL|rdkafka#producer-4| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 18 identical error(s) suppressed)\n",
      "%3|1727698029.330|FAIL|rdkafka#producer-5| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 23 identical error(s) suppressed)\n",
      "%3|1727698029.830|FAIL|rdkafka#producer-5| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 23 identical error(s) suppressed)\n",
      "%3|1727698032.330|FAIL|rdkafka#producer-5| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 18 identical error(s) suppressed)\n",
      "%3|1727698035.849|FAIL|rdkafka#producer-6| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 25 identical error(s) suppressed)\n",
      "%3|1727698036.349|FAIL|rdkafka#producer-6| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 21 identical error(s) suppressed)\n",
      "%3|1727698036.586|FAIL|rdkafka#producer-4| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 18 identical error(s) suppressed)\n",
      "%3|1727698038.352|FAIL|rdkafka#producer-6| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 16 identical error(s) suppressed)\n",
      "%3|1727698058.594|FAIL|rdkafka#producer-4| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 22 identical error(s) suppressed)\n",
      "%3|1727698060.094|FAIL|rdkafka#producer-4| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 21 identical error(s) suppressed)\n",
      "%3|1727698061.344|FAIL|rdkafka#producer-5| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 27 identical error(s) suppressed)\n",
      "%3|1727698062.344|FAIL|rdkafka#producer-5| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 16 identical error(s) suppressed)\n",
      "%3|1727698063.844|FAIL|rdkafka#producer-5| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 21 identical error(s) suppressed)\n",
      "%3|1727698066.361|FAIL|rdkafka#producer-6| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 22 identical error(s) suppressed)\n",
      "%3|1727698068.597|FAIL|rdkafka#producer-4| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 19 identical error(s) suppressed)\n",
      "%3|1727698069.363|FAIL|rdkafka#producer-6| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 16 identical error(s) suppressed)\n",
      "%3|1727698071.363|FAIL|rdkafka#producer-6| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 26 identical error(s) suppressed)\n",
      "%3|1727698090.606|FAIL|rdkafka#producer-4| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 17 identical error(s) suppressed)\n",
      "%3|1727698091.107|FAIL|rdkafka#producer-4| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 28 identical error(s) suppressed)\n",
      "%3|1727698092.357|FAIL|rdkafka#producer-5| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 17 identical error(s) suppressed)\n",
      "%3|1727698092.855|FAIL|rdkafka#producer-5| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 26 identical error(s) suppressed)\n",
      "%3|1727698094.358|FAIL|rdkafka#producer-5| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 19 identical error(s) suppressed)\n",
      "%3|1727698099.110|FAIL|rdkafka#producer-4| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 22 identical error(s) suppressed)\n",
      "%3|1727698099.374|FAIL|rdkafka#producer-6| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 26 identical error(s) suppressed)\n",
      "%3|1727698101.874|FAIL|rdkafka#producer-6| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 21 identical error(s) suppressed)\n",
      "%3|1727698102.375|FAIL|rdkafka#producer-6| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 18 identical error(s) suppressed)\n",
      "%3|1727698120.618|FAIL|rdkafka#producer-4| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 17 identical error(s) suppressed)\n",
      "%3|1727698122.867|FAIL|rdkafka#producer-5| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 21 identical error(s) suppressed)\n",
      "%3|1727698124.868|FAIL|rdkafka#producer-5| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 24 identical error(s) suppressed)\n",
      "%3|1727698125.375|FAIL|rdkafka#producer-5| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 17 identical error(s) suppressed)\n",
      "%3|1727698125.620|FAIL|rdkafka#producer-4| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 21 identical error(s) suppressed)\n",
      "%3|1727698129.122|FAIL|rdkafka#producer-4| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 22 identical error(s) suppressed)\n",
      "%3|1727698151.130|FAIL|rdkafka#producer-4| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 25 identical error(s) suppressed)\n",
      "%3|1727698152.882|FAIL|rdkafka#producer-5| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 20 identical error(s) suppressed)\n",
      "%3|1727698154.882|FAIL|rdkafka#producer-5| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 20 identical error(s) suppressed)\n",
      "%3|1727698155.631|FAIL|rdkafka#producer-4| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 17 identical error(s) suppressed)\n",
      "%3|1727698160.390|FAIL|rdkafka#producer-5| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 21 identical error(s) suppressed)\n",
      "%3|1727698161.633|FAIL|rdkafka#producer-4| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 22 identical error(s) suppressed)\n",
      "%3|1727698182.893|FAIL|rdkafka#producer-5| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 16 identical error(s) suppressed)\n",
      "%3|1727698184.143|FAIL|rdkafka#producer-4| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 19 identical error(s) suppressed)\n",
      "%3|1727698185.642|FAIL|rdkafka#producer-4| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 20 identical error(s) suppressed)\n",
      "%3|1727698185.895|FAIL|rdkafka#producer-5| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 26 identical error(s) suppressed)\n",
      "%3|1727698190.401|FAIL|rdkafka#producer-5| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 22 identical error(s) suppressed)\n",
      "%3|1727698191.644|FAIL|rdkafka#producer-4| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 23 identical error(s) suppressed)\n",
      "%3|1727698212.905|FAIL|rdkafka#producer-5| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 21 identical error(s) suppressed)\n",
      "%3|1727698215.156|FAIL|rdkafka#producer-4| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 20 identical error(s) suppressed)\n",
      "%3|1727698215.654|FAIL|rdkafka#producer-4| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 23 identical error(s) suppressed)\n",
      "%3|1727698215.906|FAIL|rdkafka#producer-5| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 18 identical error(s) suppressed)\n",
      "%3|1727698220.415|FAIL|rdkafka#producer-5| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 20 identical error(s) suppressed)\n",
      "%3|1727698222.657|FAIL|rdkafka#producer-4| [thrd:localhost:9095/bootstrap]: localhost:9095/bootstrap: Connect to ipv4#127.0.0.1:9095 failed: Connection refused (after 0ms in state CONNECT, 18 identical error(s) suppressed)\n",
      "%3|1727698242.917|FAIL|rdkafka#producer-5| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 22 identical error(s) suppressed)\n",
      "%3|1727698245.667|FAIL|rdkafka#producer-4| [thrd:localhost:9094/bootstrap]: localhost:9094/bootstrap: Connect to ipv4#127.0.0.1:9094 failed: Connection refused (after 0ms in state CONNECT, 14 identical error(s) suppressed)\n",
      "%3|1727698246.919|FAIL|rdkafka#producer-5| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 25 identical error(s) suppressed)\n",
      "%3|1727698247.668|FAIL|rdkafka#producer-4| [thrd:localhost:9093/bootstrap]: localhost:9093/bootstrap: Connect to ipv4#127.0.0.1:9093 failed: Connection refused (after 0ms in state CONNECT, 23 identical error(s) suppressed)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    global producer  # Ensuring to access the global producer\n",
    "    try:\n",
    "        log_file, log_pos = load_checkpoint()\n",
    "        if log_file is None or log_pos is None:\n",
    "            log_file = binlog_starting_file\n",
    "            log_pos = 4\n",
    "    except Exception as e:\n",
    "        log_file = binlog_starting_file\n",
    "        log_pos = 4\n",
    "\n",
    "    error_count = 0\n",
    "\n",
    "    # Initializing Kafka Producer\n",
    "    try:\n",
    "\n",
    "        producer = initialize_producer()\n",
    "        time.sleep(7)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to initialize producer: {e}\")\n",
    "        return  # Exit if producer initialization fails\n",
    "    \n",
    "    while True:\n",
    "        try:                \n",
    "            stream = read_binlog(log_file, log_pos)\n",
    "         \n",
    "            for binlog_event in stream:\n",
    "                if isinstance(binlog_event, RotateEvent):\n",
    "                    current_log_file = log_file\n",
    "                    next_log_file = binlog_event.next_binlog\n",
    "                    \n",
    "                    # Adjust log position only if it is a true rotation\n",
    "                    if current_log_file == next_log_file:\n",
    "                        # Stay on the same log file and keep the position\n",
    "                        log_pos = log_pos\n",
    "                    else:\n",
    "                        # Move to the next log file and reset position\n",
    "                        log_file = next_log_file\n",
    "                        log_pos = 4\n",
    "\n",
    "                        save_checkpoint(log_file, log_pos)\n",
    "\n",
    "                elif isinstance(binlog_event, (WriteRowsEvent, UpdateRowsEvent, DeleteRowsEvent)):\n",
    "                    table = binlog_event.table\n",
    "                    schema = binlog_event.schema\n",
    "                    \n",
    "                    # Initialize columns to avoid reference before assignment\n",
    "                    columns = []\n",
    "\n",
    "                    # Attempt to create topic\n",
    "                    try: \n",
    "                        create_topic_if_not_exists(schema, table)   \n",
    "                        columns = get_column_names_from_schema(schema, table)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Topic creation error: {e}\")\n",
    "                        # Checking Kafka health\n",
    "                        if not check_kafka_cluster_health(kafka_cnf):\n",
    "                            logger.critical(\"Kafka health check failed. Exiting...\")\n",
    "                            return\n",
    "                            \n",
    "\n",
    "                    for row in binlog_event.rows:\n",
    "                        if isinstance(binlog_event, WriteRowsEvent):\n",
    "                            row_valuesI = list(row['values'].values())  # Get only the values\n",
    "                            row_dataI = {columns[i]: datetime_to_str(v) for i, v in enumerate(row_valuesI)}\n",
    "\n",
    "                            \n",
    "                            if row_dataI:\n",
    "                                message = json.dumps({\n",
    "                                    'type': 'INSERT',\n",
    "                                    'database': schema,\n",
    "                                    'table': table,\n",
    "                                    \"data\": row_dataI  # Use row_data which has datetime converted\n",
    "                                })\n",
    "                                topic = sanitize_topic_name(f\"{schema}.{table}\")\n",
    "                                \n",
    "                                produce_message(topic, message)\n",
    "\n",
    "                        elif isinstance(binlog_event, UpdateRowsEvent):\n",
    "                            row_valuesB=list(row['before_values'].values())\n",
    "                            row_valuesA=list(row['after_values'].values())\n",
    "                            \n",
    "                            \n",
    "                            before_data = {columns[i]: datetime_to_str(v) for i, v in enumerate(row_valuesB)}\n",
    "                            after_data = {columns[i]: datetime_to_str(v) for i, v in enumerate(row_valuesA)}\n",
    "\n",
    "                            if after_data:\n",
    "                                message = json.dumps({\n",
    "                                    \"type\": \"UPDATE\",\n",
    "                                    \"database\": schema,\n",
    "                                    \"table\": table,\n",
    "                                    \"before\": before_data,\n",
    "                                    \"after\": after_data\n",
    "                                })\n",
    "                                topic = sanitize_topic_name(f\"{schema}.{table}\")\n",
    "            \n",
    "                                produce_message(topic, message)\n",
    "\n",
    "\n",
    "                        elif isinstance(binlog_event, DeleteRowsEvent):\n",
    "                            row_valuesD=list(row['values'].values())\n",
    "                            deleted_data = {columns[i]: datetime_to_str(v) for i, v in enumerate(row_valuesD)}\n",
    "\n",
    "                            if deleted_data:\n",
    "                                message = json.dumps({\n",
    "                                    \"type\": \"DELETE\",\n",
    "                                    \"database\": schema,\n",
    "                                    \"table\": table,\n",
    "                                    \"data\": deleted_data\n",
    "                                })\n",
    "\n",
    "                                topic = sanitize_topic_name(f\"{schema}.{table}\")\n",
    "                                \n",
    "                                produce_message(topic, message)\n",
    "\n",
    "                        log_pos = binlog_event.packet.log_pos\n",
    "                        save_checkpoint(log_file, log_pos)\n",
    "\n",
    "                elif isinstance(binlog_event, QueryEvent):\n",
    "                    query = binlog_event.query\n",
    "                    schema = binlog_event.schema.decode() if isinstance(binlog_event.schema, bytes) else binlog_event.schema\n",
    "\n",
    "                    if any(keyword in query.upper() for keyword in [\"ALTER TABLE\", \"CREATE TABLE\", \"DROP TABLE\", \"INSERT INTO\", \"UPDATE\", \"DELETE FROM\"]):\n",
    "                        extracted_schema, extracted_table = extract_table_and_schema(query)\n",
    "                        schema = extracted_schema if extracted_schema != 'unknown_schema' else schema\n",
    "\n",
    "                        try:\n",
    "                            create_topic_if_not_exists(schema, extracted_table)                            \n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"Topic creation error: {e}\")\n",
    "                            # Checking Kafka health\n",
    "                            if not check_kafka_cluster_health(kafka_cnf):\n",
    "                                logger.critical(\"Kafka health check failed. Exiting...\")\n",
    "                                return\n",
    "\n",
    "                        message = json.dumps({\n",
    "                            'type': 'QUERY',\n",
    "                            'database': schema,\n",
    "                            'table': extracted_table,\n",
    "                            'query': query\n",
    "                        })\n",
    "\n",
    "                        topic = sanitize_topic_name(f\"{schema}.{extracted_table}\")\n",
    "                        \n",
    "                        produce_message(topic, message)\n",
    "\n",
    "                        log_pos = binlog_event.packet.log_pos\n",
    "                        save_checkpoint(log_file, log_pos)\n",
    "            \n",
    "                error_count=0\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            logger.error(f\"An error occurred: {e}\")\n",
    "\n",
    "            if error_count == 5:  # Increased threshold\n",
    "                detailed_error_message = f\"Error occurred: {e}, Log file: {log_file}, Log position: {log_pos}\"\n",
    "                send_email_alert(\"Error occurred in processing\", detailed_error_message)\n",
    "                return\n",
    "                \n",
    "        finally:\n",
    "            stream.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9723ecd8-8642-49bc-85a1-6ce8e41f4a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
