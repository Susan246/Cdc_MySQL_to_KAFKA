{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca08af7-777b-4c5e-8318-d15306760244",
   "metadata": {},
   "source": [
    "# Capturing Data Change of MySQL from Binlog and passing to Kafka (Producer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072da330-ed8a-4334-a6d9-d44645004c64",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "859f829d-6111-45cc-8d9b-a758f78efacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "from pymysqlreplication import BinLogStreamReader\n",
    "from pymysqlreplication.row_event import DeleteRowsEvent,UpdateRowsEvent,WriteRowsEvent\n",
    "from pymysqlreplication.event import QueryEvent,RotateEvent\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.admin import AdminClient,NewTopic\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import subprocess\n",
    "import socket\n",
    "import threading\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from typing import Tuple, Optional,List\n",
    "from datetime import datetime,date,timedelta,time as dt_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e415550e-0dce-4055-996c-afd2f6a9bd34",
   "metadata": {},
   "source": [
    "### configurations used in our code\n",
    "- change it as per your requirements, like bootstrap.servers, zookeeper_cnf, mysql_cnf and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f3c2fc-c13a-42f3-a6d8-ac82866fee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka configuration\n",
    "kafka_cnf={\n",
    "    'bootstrap.servers':'localhost:9092',       # Kafka broker(s) address. This is the Kafka server IP and port (can refer to server.properties file).\n",
    "    'acks':'all',                               # Acknowledgment setting. 'all' ensures that the producer waits for the leader and all in-sync replicas to acknowledge the message, guaranteeing durability.\n",
    "    'enable.idempotence':True,                  # Ensures the producer does not write duplicate messages in case of retries. With this enabled, Kafka can detect duplicates and avoid duplication.\n",
    "    'retry.backoff.ms':100,                     # Time in milliseconds to wait before retrying after a request failure. Setting this lower helps retry failed requests faster, in case of transient failures.\n",
    "    'retries':'3',                              # Number of times the producer retries sending a message if a transient failure occurs. Setting this to a lower value prevents excessive retries in case of persistent issues.\n",
    "    'retry.backoff.max.ms':2000,                # Maximum time in milliseconds between retry attempts. This helps prevent fast retries in case of errors, avoiding overwhelming the broker if the error persists.\n",
    "    'delivery.timeout.ms':30000,                # Total time (in milliseconds) allowed for a message to be delivered. If the message is not delivered within this time, it will fail.\n",
    "    'request.timeout.ms':20000,                 # Time (in milliseconds) to wait for the broker's response to a request. If no response is received within this period, the request will fail and may trigger a retry.\n",
    "    'max.in.flight.requests.per.connection':5,  # Maximum number of unacknowledged requests allowed per connection. Limiting this ensures retries in case of failure without breaking message ordering (important when idempotence is enabled).\n",
    "    'batch.size':1048576,                       # The size of batches to be sent to the broker in bytes (1 MB in this case). Larger batches can improve throughput by sending more messages in a single request.\n",
    "    'linger.ms':1,                              # The time (in milliseconds) the producer waits for more records to be sent before batching and sending to the broker. A small value (like 3 ms) reduces latency while allowing a slight delay for batching.\n",
    "    # 'buffer.memory':67108864,                 # (confluent_kafka client doesn't support I don't know why) The amount of memory the producer can use to buffer records before sending them to the broker. Uncomment this if you want to control the buffer memory (default is 32 MB).\n",
    "    'compression.type':'snappy'                 # Compression algorithm to use for message batches. 'snappy' offers a good tradeoff between compression speed and size, which is generally suitable for high throughput.\n",
    "}\n",
    "\n",
    "\n",
    "# zookeeper configuration\n",
    "zookeeper_cnf = {\n",
    "    'host': 'localhost',            # zookeeper server ip:port , you can refer your zookeeper.properties file\n",
    "    'port': 2181\n",
    "}\n",
    "\n",
    "# mysql configuration\n",
    "mysql_cnf={\n",
    "    'host':'localhost',            # mysql server ip, port number, user name and user password  \n",
    "    'port':3308,                   # and make sure that the user have required privileges\n",
    "    'user':'root',                 # in our case make [binlog_format='MIXED', binlog_row_image='FULL' and binlog_row_metadata='FULL']\n",
    "    'passwd':'Root@123'\n",
    "}\n",
    "\n",
    "# mysql_server id, change it to yours\n",
    "mysql_server_id=2\n",
    "\n",
    "# binlog starting file from which we starts to read binlog, you can change as per your requirements\n",
    "binlog_starting_file='binlog.000001'  \n",
    "\n",
    "# log file path, change it to your appropriate path\n",
    "log_file_path='/media/susan/F4707AD3707A9BD4/MysqlCdc_Python_kafka/single_node_kafka_and_zookeeper/examples/cdc.log'\n",
    "\n",
    "# Checkpointing mechanism and file path to save, change it to your appropriate path\n",
    "checkpoint_file ='/media/susan/F4707AD3707A9BD4/MysqlCdc_Python_kafka/single_node_kafka_and_zookeeper/examples/checkpoint.json'\n",
    "\n",
    "# file path of the file which is required for checking status of kafka \n",
    "kafka_broker_api_versions='/media/susan/F4707AD3707A9BD4/MysqlCdc_Python_kafka/single_node_kafka_and_zookeeper/bin/kafka-broker-api-versions'\n",
    "\n",
    "# for email alerting, change it to yours\n",
    "sender_email = os.getenv('EMAIL_USER')    # sender_email address,receiver_email address and password of sender_email address \n",
    "receiver_email = os.getenv('EMAIL_USER')  # I just give from environment variable and for password use app passport for email (recommended)\n",
    "password = os.getenv('EMAIL_PASSWORD') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6935502-5cef-4d1b-80c1-09582e05416e",
   "metadata": {},
   "source": [
    "### initializing logging configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104fd4be-b5e4-4f55-b6c2-e617fe8eabc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up custom logger\n",
    "logger = logging.getLogger('my_logger')     # you can give any name as per your requirement\n",
    "logger.setLevel(logging.INFO)               # Setting the logging level, here highest level is info and only that level messages are logged \n",
    "\n",
    "# setting custom File handler that log messages to a file\n",
    "file_handler = logging.FileHandler(log_file_path)\n",
    "file_handler.setLevel(logging.INFO)         # Log level for file handler\n",
    "\n",
    "# setting custom Formatter to define the log message format\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')  # it log in the format [time_of_logging  level_of_log  actual_messages]\n",
    "\n",
    "# adding that custom formatter to file handler\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# setting propagate to False to prevent messages from being sent to the root logger, cause sometimes root logger prints the log messages in console so\n",
    "logger.propagate = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c461e2d-51d2-4f7d-ad46-ed626e3129b5",
   "metadata": {},
   "source": [
    "### this function is responsible for reading binlog from mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4593f8b6-12af-46c9-b5e9-34b5de176c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_binlog(log_file: str, log_pos: int, retries: int = 3, backoff: int = 5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to read a MySQL binlog starting from a specific file and position, with retry logic.\n",
    "    \n",
    "    Args:\n",
    "    - log_file (str): The name of the binlog file to read from.\n",
    "    - log_pos (int): The position in the binlog file to start reading from.\n",
    "    - retries (int): Number of retry attempts in case of failure (default: 3).\n",
    "    - backoff (int): Time (in seconds) to wait between retries (default: 5 seconds).\n",
    "\n",
    "    Returns:\n",
    "    - BinLogStreamReader object if successful, otherwise logs error after retries are exhausted.\n",
    "    \"\"\"\n",
    "    \n",
    "    attempt=0\n",
    "    while attempt<retries:\n",
    "        try:\n",
    "            # initializing a BinLogStreamReader to read binlog events\n",
    "            stream=BinLogStreamReader(                                  \n",
    "                connection_settings=mysql_cnf,\n",
    "                server_id=mysql_server_id,\n",
    "                blocking=True,\n",
    "                resume_stream=True,\n",
    "                only_events=[WriteRowsEvent, UpdateRowsEvent, DeleteRowsEvent, QueryEvent, RotateEvent],\n",
    "                log_file=log_file,\n",
    "                log_pos=log_pos\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Successfully read binlog from file '{log_file}' at position '{log_pos}'.\")\n",
    "            return stream\n",
    "        except Exception as e:\n",
    "            attempt+=1\n",
    "\n",
    "            if attempt<retries:\n",
    "                logger.info(f\"Retrying to read binlog from '{log_file}' at position '{log_pos}'... (Attempt {attempt}/{retries})\")\n",
    "                time.sleep(backoff)\n",
    "            else:\n",
    "                logger.error(f\"Max retries reached. Binlog reading from '{log_file}' at position '{log_pos}' failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec2baec-8fb1-4937-a4e0-158e797b06aa",
   "metadata": {},
   "source": [
    "### following two functions are responsible for checkpoint mechanism\n",
    "- load_checkpoint() is responsible for loading last saved binlog file and position\n",
    "- save_checkpoint() is responsible for saving last processed binlog file and position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5625a6d1-25ae-4324-85b5-f7e00dda46bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint() -> Tuple[Optional[str], Optional[int]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Function to load the last saved binlog position (checkpoint) from a file (checkpoint_file).\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple (log_file, log_pos) if a valid checkpoint is found.\n",
    "    - (None, None) if the checkpoint file is missing, empty, or corrupted.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the checkpoint file exists and is not empty\n",
    "    if os.path.exists(checkpoint_file) and os.path.getsize(checkpoint_file) > 0:\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            try:\n",
    "                checkpoint = json.load(f)\n",
    "                return checkpoint['log_file'], checkpoint['log_pos']\n",
    "            except json.JSONDecodeError:\n",
    "                # Handle invalid JSON or empty file\n",
    "                logger.error(\"Checkpoint file is corrupted. Starting from the beginning.\")\n",
    "                return None, None\n",
    "    else:\n",
    "        # No checkpoint file or the file is empty\n",
    "        logger.info(\"No valid checkpoint found. Starting from the beginning.\")\n",
    "        return None, None\n",
    "        \n",
    "\n",
    "def save_checkpoint(log_file: str, log_pos: int) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function to save the current binlog file and position to a checkpoint file.\n",
    "    \n",
    "    Args:\n",
    "    - log_file (str): The current binlog file name.\n",
    "    - log_pos (int): The current position in the binlog file.\n",
    "    \"\"\"\n",
    "\n",
    "    # creating a dictionary to store the log file and position\n",
    "    checkpoint = {\n",
    "        'log_file': log_file,\n",
    "        'log_pos': log_pos\n",
    "    }\n",
    "\n",
    "    # writing the checkpoint data to the checkpoint file in JSON format\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(checkpoint, f)   # serializing the dictionary into json format and writing into file\n",
    "        logger.info(f\"Checkpoint saved: log_file='{log_file}', log_pos={log_pos}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab53715-aff7-4258-a43c-2f8d53c0a36e",
   "metadata": {},
   "source": [
    "### following functions:\n",
    "- sanitize_topic_name() is responisble for making appropriate topic name\n",
    "- create_topic_if_not_exists() is responsible for making kafka topics dynamically/automatically\n",
    "-  datetime_to_str() is responsible for converting data related data into string cause json serializaiton doesn't support date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be8b6ffa-bcd5-40fd-9303-558b9c21d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_topic_name(topic_name: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Function to sanitize Kafka topic names by replacing invalid characters.\n",
    "\n",
    "    Args:\n",
    "    - topic_name (str): The original topic name.\n",
    "\n",
    "    Returns:\n",
    "    - sanitized_name (str): The sanitized topic name with a max length of 255 characters.\n",
    "    \"\"\"\n",
    "\n",
    "    # replacing all characters that are not allowed in Kafka topic names (anything except letters, digits, '_', '-', or '.') with underscores.\n",
    "    sanitized_name = re.sub(r'[^a-zA-Z0-9._-]', '_',topic_name)\n",
    "    return sanitized_name[:255]\n",
    "\n",
    "\n",
    "\n",
    "def create_topic_if_not_exists(database: str, table: str) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function to create a Kafka topic if it does not already exist.\n",
    "    \n",
    "    Args:\n",
    "    - database (str): The name of the database.\n",
    "    - table (str): The name of the table.\n",
    "    \n",
    "    The function first sanitizes the topic name, checks if it exists, \n",
    "    and if not, creates a new topic with one partition and a replication factor of 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    topic_name=f\"{database}.{table}\"\n",
    "    topic_name = sanitize_topic_name(topic_name)  # ensuring topic names are sanitized\n",
    "    admin_client = AdminClient(kafka_cnf)         # initializing AdminClinet to manage kafka topics\n",
    "\n",
    "    existing_topics=admin_client.list_topics(timeout=10).topics.keys()   # retriveing the list of existing kafka topics\n",
    "\n",
    "    # checking the topic is already exists or not\n",
    "    if topic_name not in existing_topics:\n",
    "        try:\n",
    "            # creating a new kafka topic\n",
    "            new_topic=admin_client.create_topics(\n",
    "                [NewTopic(topic_name,num_partitions=1,replication_factor=1)]\n",
    "            )\n",
    "            # ensuring the creation of the topic is successful by checking the result\n",
    "            new_topic[topic_name].result()\n",
    "            logger.info(f\"Topic '{topic_name}' is created.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating topic '{topic_name}': {e}\")\n",
    "    else:\n",
    "        logger.info(f\"Topic '{topic_name}' already exists.\")\n",
    "\n",
    "\n",
    "\n",
    "def datetime_to_str(dt):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to convert various datetime-related objects to a string format.\n",
    "\n",
    "    Args:\n",
    "    - dt: The datetime-related object to convert.\n",
    "\n",
    "    Returns:\n",
    "    - A string representation of the datetime, date, time, or timedelta object.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(dt, datetime):        # Full datetime with date and time\n",
    "        return dt.isoformat()           # returns ISO 8601 string format\n",
    "    elif isinstance(dt, date):          # Date without time\n",
    "        return dt.isoformat()\n",
    "    elif isinstance(dt, dt_time):       # Time only (hours, minutes, seconds)\n",
    "        return dt.strftime('%H:%M:%S')  # Format time as string\n",
    "    elif isinstance(dt, timedelta):     # Handle timedelta\n",
    "        total_seconds = int(dt.total_seconds())\n",
    "        hours, remainder = divmod(total_seconds, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        return f\"{hours:02}:{minutes:02}:{seconds:02}\"  # Formatting as HH:MM:SS\n",
    "    else:\n",
    "        return dt  # Return unchanged if not a datetime, date, time, or timedelta object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df5811b-7d90-40d8-9690-a27ddc142f55",
   "metadata": {},
   "source": [
    "### following functions:\n",
    "- initialize_producer() is responsible for initializing kafka producer\n",
    "- produce_message() is responsible for producing messages to kafka topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f95d48fa-09c6-4485-aeea-fbc21c00e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = None  # initializing the producer variable globally\n",
    "\n",
    "def initialize_producer(retries: int = 5, base_backoff: int = 2) -> Producer:\n",
    "\n",
    "    \"\"\"\n",
    "    Initialize Kafka Producer with exponential backoff retry mechanism.\n",
    "\n",
    "    Args:\n",
    "        retries: Number of times to retry initialization if it fails.\n",
    "        base_backoff: Base time in seconds for the exponential backoff between retries.\n",
    "\n",
    "    Returns:\n",
    "        Producer: An instance of the initialized Kafka producer.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If max retries are reached and producer initialization fails.\n",
    "    \"\"\"\n",
    "    \n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            producer = Producer(**kafka_cnf)  # initializing kafka producer\n",
    "            logger.info(\"Kafka producer initialized successfully.\")\n",
    "            # print('kafka producer initialized successfully')\n",
    "            return producer\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            if attempt < retries:\n",
    "                backoff_time = base_backoff * (2 ** (attempt - 1))  # applying exponential backoff\n",
    "                logger.info(f\"Retrying to initialize producer in {backoff_time} seconds... ({attempt}/{retries})\")\n",
    "                time.sleep(backoff_time)\n",
    "            else:\n",
    "                logger.critical(\"Max retries reached for producer initialization.\")\n",
    "                send_email_email('Failed Producer initialization Alert','Failed to initialize producer and max retires reached to initialization')\n",
    "                raise\n",
    "\n",
    "\n",
    "def produce_message(topic: Optional[str], message: Optional[str], retries: int = 3, backoff: int = 2) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Produce a message to a specified Kafka topic with retry mechanism.\n",
    "\n",
    "    Args:\n",
    "        topic: The Kafka topic where the message will be sent.\n",
    "        message: The message content to be produced.\n",
    "        retries: Number of retries in case of failure to produce the message.\n",
    "        backoff: Base backoff time in seconds before each retry.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    global producer  # Ensuring to use the global producer\n",
    "    if producer is None:\n",
    "        logger.error(\"Producer not initialized. Cannot produce message.\")\n",
    "        return  # Exit if producer is not initialized\n",
    "    \n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            # producing messages to the kafka topic\n",
    "            producer.produce(topic, value=message)  \n",
    "            producer.flush()  # ensuring the message is sent\n",
    "            logger.info(f\"Message: {message} is successfully produced to topic '{topic}'.\")\n",
    "            # print(f\"Message successfully produced to topic '{topic}'.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to produce message to topic '{topic}':\\n Message content: '{message} \\n error:{e}'\")\n",
    "            # Check if the error is related to PID acquisition\n",
    "            if \"Failed to acquire idempotence PID\" in str(e):\n",
    "                logger.info(\"Encountered PID acquisition issue; retrying...\")\n",
    "                attempt += 1\n",
    "                time.sleep(backoff)  # Wait before retrying\n",
    "            else:\n",
    "                attempt += 1  # Increment for other types of errors\n",
    "                if attempt < retries:\n",
    "                    logger.info(f\"Retrying to produce message to topic '{topic}'... Attempt {attempt} of {retries}.\")\n",
    "                    time.sleep(backoff)\n",
    "                else:\n",
    "                    logger.critical(\"Max retries reached. Message production failed.\")\n",
    "                    send_email_alert('Message Production Failure Alert', \n",
    "                            f\"Failed to produce message to topic '{topic}'.\\n\"\n",
    "                            f\"Message Content: '{message}'.\\n\"\n",
    "                            f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4528dc25-3d6c-4573-9291-ad516faa18d0",
   "metadata": {},
   "source": [
    "### following function is responsible for extracting database and table name, we need it for extracting columns name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea21c8c-5d9a-4b35-86b2-27bbb301124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_and_schema(query):\n",
    "\n",
    "    \"\"\"\n",
    "    Extract the schema and table names from a given SQL query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The SQL query string from which to extract schema and table names.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the schema name and table name.\n",
    "               If not found, it returns ('unknown_schema', 'unknown_table').\n",
    "    \"\"\"\n",
    "    \n",
    "    # regex pattern to capture schema and table names from various SQL commands\n",
    "    match = re.search(\n",
    "        r\"(ALTER TABLE|CREATE TABLE|DROP TABLE|INSERT INTO|UPDATE|DELETE FROM)\\s+\"\n",
    "        r\"(?:`?(\\w+)`?\\.)?`?(\\w+)`?\", query, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        # capture the schema name (if provided) or set to 'unknown_schema'\n",
    "        schema = match.group(2) if match.group(2) else 'unknown_schema'  \n",
    "        # capture the table name\n",
    "        table = match.group(3)\n",
    "        return schema, table\n",
    "\n",
    "    # If match isn't found, return default values\n",
    "    return 'unknown_schema', 'unknown_table'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d98633-6b54-4ad1-a1f6-61e4f2f195b2",
   "metadata": {},
   "source": [
    "### following function is responsible for extracting columns name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ce0ac13-cc9f-473f-a2de-77c26b4e49ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names_from_schema(schema: str, table: str) -> List[str]:\n",
    "\n",
    "    \"\"\"\n",
    "    Retrieve column names from a specified table in the given schema.\n",
    "\n",
    "    Args:\n",
    "        schema (str): The database schema (database name).\n",
    "        table (str): The table name from which to retrieve column names.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of column names from the specified table, or an empty list if an error occurs.\n",
    "    \"\"\"\n",
    "\n",
    "    # creating a copy of mysql_cnf to avoid modifying the global variable\n",
    "    local_mysql_cnf = copy.deepcopy(mysql_cnf)\n",
    "    local_mysql_cnf['database'] = schema  # adding database \n",
    "\n",
    "    try:\n",
    "        # connectioning mysql server\n",
    "        with pymysql.connect(**local_mysql_cnf) as connection:\n",
    "            with connection.cursor() as cursor:\n",
    "                query = \"\"\"\n",
    "                    SELECT COLUMN_NAME \n",
    "                    FROM INFORMATION_SCHEMA.COLUMNS \n",
    "                    WHERE TABLE_SCHEMA = %s AND TABLE_NAME = %s\n",
    "                    ORDER BY ORDINAL_POSITION  -- Ensures the columns are in the defined order\n",
    "                \"\"\"\n",
    "                # retrieving the columns name\n",
    "                cursor.execute(query, (schema, table))\n",
    "                column_names = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "                return column_names\n",
    "\n",
    "    except pymysql.MySQLError as e:\n",
    "        logger.error(f\"Error retrieving column names from {schema}.{table}: {e}\")\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f01b3f-63fd-40b8-80b1-57b91807d191",
   "metadata": {},
   "source": [
    "### following functions;\n",
    "- send_email_alert() responsible for sending alert emails\n",
    "- check_mysql_health() responsible for status checking of mysql server; it is running or not\n",
    "- check_kafka_health() is responsible for status checking of kafka server\n",
    "- check_zookeeper_health() is responsible for status chechking of zookeeper server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "095e0561-10fa-4ea0-81c4-c81f71810151",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def send_email_alert(subject, message):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to send an email alert.\n",
    "\n",
    "    Args:\n",
    "        subject (str): The subject line of the email.\n",
    "        message (str): The content of the email message.\n",
    "    \n",
    "    This function retrieves sender and receiver email addresses and the password from \n",
    "    environment variables for security. It creates an email message and uses an SMTP \n",
    "    server to send the email alert.\n",
    "    \"\"\"\n",
    "    \n",
    "    from_email=sender_email\n",
    "    to_email=receiver_email \n",
    "    passwd = password\n",
    "    \n",
    "    msg = MIMEText(message)    # email message\n",
    "    msg['Subject'] = subject   # email subject\n",
    "    msg['From'] = from_email\n",
    "    msg['To'] = to_email\n",
    "    \n",
    "    try:\n",
    "        # sending alert email\n",
    "        with smtplib.SMTP('smtp.gmail.com', 587) as server:  # Update with your SMTP server\n",
    "            server.starttls()\n",
    "            server.login(from_email, passwd)\n",
    "            server.sendmail(from_email, to_email, msg.as_string())\n",
    "            logger.info(\"Alert email sent successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to send email: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "def check_mysql_health(mysql_cnf, retries=5, delay=10):\n",
    "    \"\"\"\n",
    "    Function to check MySQL server health with retry mechanism.\n",
    "\n",
    "    Args:\n",
    "        mysql_cnf (dict): A dictionary containing MySQL connection parameters (user, password, host, port).\n",
    "        retries (int): Number of retry attempts if MySQL server is down.\n",
    "        delay (int): Delay (in seconds) between retry attempts.\n",
    "    \n",
    "    This function attempts to ping the MySQL server using the provided configuration and\n",
    "    retries if the server is down. Returns True if the server is healthy, otherwise False\n",
    "    after all retries.\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            # Executing the mysqladmin command to ping the MySQL server\n",
    "            result = subprocess.run(\n",
    "                [\n",
    "                    'mysqladmin',               # Command to interact with MySQL server\n",
    "                    '-u', mysql_cnf['user'], \n",
    "                    '-p' + mysql_cnf['passwd'], \n",
    "                    '-h', mysql_cnf['host'], \n",
    "                    '-P', str(mysql_cnf['port']),\n",
    "                    'ping'                     # Command to check if the server is alive\n",
    "                ],\n",
    "                stdout=subprocess.PIPE, stderr=subprocess.PIPE   # Capture standard output and error\n",
    "            )\n",
    "\n",
    "            # Checking if the command executed successfully\n",
    "            if result.returncode == 0:\n",
    "                logger.info(f\"MySQL server {mysql_cnf['host']}:{mysql_cnf['port']} is healthy.\")\n",
    "                return True  # Server is healthy\n",
    "            else:\n",
    "                # Raise an exception if the command fails\n",
    "                raise Exception(result.stderr.decode())\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Attempt {attempt}: MySQL check failed for server {mysql_cnf['host']}:{mysql_cnf['port']}: {str(e)}\")\n",
    "            \n",
    "            # If the number of retries is reached, send an alert\n",
    "            if attempt == retries:\n",
    "                error_message = f\"MySQL server {mysql_cnf['host']}:{mysql_cnf['port']} is down after {retries} attempts.\"\n",
    "                subject = \"MySQL Health Alert\"\n",
    "                send_email_alert(subject, error_message)  # Send email alert\n",
    "                logger.critical(f\"MySQL server {mysql_cnf['host']}:{mysql_cnf['port']} is down after {retries} retries.\")\n",
    "                return False\n",
    "            else:\n",
    "                logger.warning(f\"MySQL server {mysql_cnf['host']}:{mysql_cnf['port']} is still down. Retrying in {delay} seconds... (Attempt {attempt+1}/{retries})\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "\n",
    "\n",
    "def check_kafka_health(kafka_cnf, retries=5, delay=10):\n",
    "    \"\"\"\n",
    "    Function to check Kafka broker health with retry mechanism.\n",
    "\n",
    "    Args:\n",
    "        kafka_cnf (dict): A dictionary containing Kafka connection parameters (e.g., bootstrap.servers).\n",
    "        retries (int): Number of retry attempts if Kafka broker is down.\n",
    "        delay (int): Delay (in seconds) between retry attempts.\n",
    "    \n",
    "    This function attempts to check the health of the Kafka broker by executing a command\n",
    "    to retrieve broker API versions. It retries the check up to 'retries' times if the broker is down.\n",
    "    Returns True if the broker is healthy, otherwise False after all retries.\n",
    "    \"\"\"\n",
    "    \n",
    "    kafka_server = kafka_cnf.get('bootstrap.servers')\n",
    "    \n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            # Execute the command to check Kafka broker API versions\n",
    "            result = subprocess.run(\n",
    "                [kafka_broker_api_versions,\n",
    "                 '--bootstrap-server', kafka_server],\n",
    "                stdout=subprocess.PIPE, stderr=subprocess.PIPE  # Capture output and error\n",
    "            )\n",
    "            \n",
    "            # Check if the command executed successfully\n",
    "            if result.returncode == 0:\n",
    "                logger.info(f\"Kafka broker {kafka_server} is healthy.\")\n",
    "                return True  # Broker is healthy\n",
    "            else:\n",
    "                # Raise an exception if the command fails\n",
    "                raise Exception(result.stderr.decode())\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Attempt {attempt}: Kafka check failed for broker {kafka_server}: {str(e)}\")\n",
    "            \n",
    "            # If the number of retries is reached, send an alert\n",
    "            if attempt == retries:\n",
    "                error_message = f\"Kafka broker {kafka_server} is down after {retries} attempts.\"\n",
    "                subject = \"Kafka Health Alert\"\n",
    "                send_email_alert(subject, error_message)  # Send an email alert with broker info\n",
    "                logger.critical(f\"Kafka broker {kafka_server} is down after {retries} retries.\")\n",
    "                return False\n",
    "            else:\n",
    "                logger.warning(f\"Kafka broker {kafka_server} is still down. Retrying in {delay} seconds... (Attempt {attempt+1}/{retries})\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "\n",
    "\n",
    "def check_zookeeper_health(zookeeper_cnf, retries=5, delay=10):\n",
    "    \"\"\"\n",
    "    Function to check Zookeeper server health with retry mechanism.\n",
    "\n",
    "    Args:\n",
    "        zookeeper_cnf (dict): A dictionary containing Zookeeper connection parameters (host, port).\n",
    "        retries (int): Number of retry attempts if Zookeeper server is down.\n",
    "        delay (int): Delay (in seconds) between retry attempts.\n",
    "    \n",
    "    This function attempts to connect to the Zookeeper server using a socket and sends the 'ruok' command.\n",
    "    It retries if the server is down and returns True if the server is healthy; otherwise, returns False\n",
    "    after all retries.\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            # Retrieving the Zookeeper host and port from the configuration\n",
    "            zk_host = zookeeper_cnf.get('host')\n",
    "            zk_port = zookeeper_cnf.get('port')\n",
    "\n",
    "            # Creating a socket connection to the Zookeeper server\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.settimeout(5)  # Set a timeout for the connection\n",
    "                s.connect((zk_host, zk_port))  # Connect to the Zookeeper server\n",
    "                s.sendall(b'ruok\\n')  # Send the 'ruok' command\n",
    "                response = s.recv(1024).decode().strip()  # Receive and strip response\n",
    "\n",
    "            # Checking if the response is 'imok', indicating the server is healthy\n",
    "            if response.lower() == 'imok':\n",
    "                logger.info(f\"Zookeeper server {zk_host}:{zk_port} is healthy.\")\n",
    "                return True\n",
    "            else:\n",
    "                raise Exception(f\"Unexpected Zookeeper response: {response}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Attempt {attempt}: Zookeeper check failed for server {zk_host}:{zk_port}: {str(e)}\")\n",
    "            \n",
    "            # If the number of retries is reached, send an alert\n",
    "            if attempt == retries:\n",
    "                error_message = f\"Zookeeper server {zk_host}:{zk_port} is down after {retries} attempts.\"\n",
    "                subject = \"Zookeeper Health Alert\"\n",
    "                send_email_alert(subject, error_message)  # Send email alert\n",
    "                logger.critical(f\"Zookeeper server {zk_host}:{zk_port} is down after {retries} retries.\")\n",
    "                return False\n",
    "            else:\n",
    "                logger.warning(f\"Zookeeper server {zk_host}:{zk_port} is still down. Retrying in {delay} seconds... (Attempt {attempt+1}/{retries})\")\n",
    "                time.sleep(delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6097c0-44f5-4972-a913-ea9fad1ea410",
   "metadata": {},
   "source": [
    "### following function monitor_system_continuously() is responsible for checkig status of the servers; mysql, kafka, zookeeper at every hour \n",
    "- if the system is down for 3 iteration then the checking gonna stop, so for continuos 3 iteration if the servers are down then each alert email gonna trigger for each hour (i.e. after 3rd alert the checking will be stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "822328da-17ec-4cec-9133-8d1a3283695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def monitor_system_continuously(interval: int = 3600):\n",
    "\n",
    "    \"\"\"\n",
    "    Continuously monitor the system at specified intervals and send alerts if services are down.\n",
    "\n",
    "    Args:\n",
    "        interval (int): The time (in seconds) to wait between checks. Default is 3600 seconds (1 hour).\n",
    "    \n",
    "    This function performs health checks for MySQL, Kafka, and Zookeeper services up to three times. \n",
    "    If any service is found to be down, it sends alerts and logs the event. The monitoring runs in a \n",
    "    separate thread.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_checks = 3\n",
    "    total_checks = 0\n",
    "\n",
    "    while total_checks < max_checks:\n",
    "        try:\n",
    "            # checking the health of MySQL, Kafka, and Zookeeper\n",
    "            mysql_health = check_mysql_health(mysql_cnf)\n",
    "            kafka_health = check_kafka_health(kafka_cnf)\n",
    "            zookeeper_health = check_zookeeper_health(zookeeper_cnf)\n",
    "            \n",
    "            if not (mysql_health and kafka_health and zookeeper_health):\n",
    "                raise Exception(\"One or more services are down.\")\n",
    "\n",
    "            total_checks=0      # Reset successful checks counter after a successful check\n",
    "            logger.info(\"System's health check successful.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"System monitoring failed: {e}\")\n",
    "\n",
    "        total_checks += 1\n",
    "        time.sleep(interval)  # Wait for the specified interval (10 minutes) before checking again\n",
    "\n",
    "    logger.info(\"Completed three checks for each hour.\")\n",
    "\n",
    "# Start the monitoring thread when the program starts\n",
    "monitor_thread = threading.Thread(target=monitor_system_continuously, args=(3600,))  # Checks every 20 minutes\n",
    "monitor_thread.daemon = True  # Ensures the thread will exit when the main program exits\n",
    "monitor_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a8e8e-c213-465a-853f-7f2e492016d3",
   "metadata": {},
   "source": [
    "## Main function of program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b57d42f-ef63-4711-b82b-e411abb70273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1727939663.810|GETPID|rdkafka#producer-1| [thrd:main]: Failed to acquire idempotence PID from broker localhost:9092/bootstrap: Broker: Coordinator load in progress: retrying\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    global producer  # Ensuring to access the global producer\n",
    "    try:\n",
    "        log_file, log_pos = load_checkpoint()\n",
    "        if log_file is None or log_pos is None:\n",
    "            log_file = binlog_starting_file\n",
    "            log_pos = 4\n",
    "    except FileNotFoundError:\n",
    "        log_file = binlog_starting_file\n",
    "        log_pos = 4\n",
    "\n",
    "    error_count = 0\n",
    "\n",
    "    # Initializing Kafka Producer\n",
    "    try:\n",
    "\n",
    "        producer = initialize_producer()\n",
    "        time.sleep(7)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to initialize producer: {e}\")\n",
    "        return  # Exit if producer initialization fails\n",
    "    \n",
    "    while True:\n",
    "        try:                \n",
    "            stream = read_binlog(log_file, log_pos)\n",
    "\n",
    "            for binlog_event in stream:\n",
    "                if isinstance(binlog_event, RotateEvent):\n",
    "                    current_log_file = log_file\n",
    "                    next_log_file = binlog_event.next_binlog\n",
    "                    \n",
    "                    # Adjust log position only if it is a true rotation\n",
    "                    if current_log_file == next_log_file:\n",
    "                        # Stay on the same log file and keep the position\n",
    "                        log_pos = log_pos\n",
    "                    else:\n",
    "                        # Move to the next log file and reset position\n",
    "                        log_file = next_log_file\n",
    "                        log_pos = 4\n",
    "\n",
    "                        save_checkpoint(log_file, log_pos)\n",
    "\n",
    "                elif isinstance(binlog_event, (WriteRowsEvent, UpdateRowsEvent, DeleteRowsEvent)):\n",
    "                    table = binlog_event.table\n",
    "                    schema = binlog_event.schema\n",
    "                    \n",
    "                    # Initialize columns to avoid reference before assignment\n",
    "                    columns = []\n",
    "\n",
    "                    # Attempt to create topic\n",
    "                    try:\n",
    "                        create_topic_if_not_exists(schema, table)\n",
    "                        columns = get_column_names_from_schema(schema, table)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Topic creation error: {e}\")\n",
    "                        # Checking Kafka health\n",
    "                        if not check_kafka_health(kafka_cnf):\n",
    "                            logger.critical(\"Kafka health check failed. Exiting...\")\n",
    "                            return\n",
    "\n",
    "                    for row in binlog_event.rows:\n",
    "                        if isinstance(binlog_event, WriteRowsEvent):\n",
    "                            row_valuesI = list(row['values'].values())  # Get only the values\n",
    "                            row_dataI = {columns[i]: datetime_to_str(v) for i, v in enumerate(row_valuesI)}\n",
    "\n",
    "                            \n",
    "                            if row_dataI:\n",
    "                                message = json.dumps({\n",
    "                                    'type': 'INSERT',\n",
    "                                    'database': schema,\n",
    "                                    'table': table,\n",
    "                                    \"data\": row_dataI  # Use row_data which has datetime converted\n",
    "                                })\n",
    "                                \n",
    "                                topic = sanitize_topic_name(f\"{schema}.{table}\")\n",
    "                                produce_message(topic, message)\n",
    "\n",
    "                        elif isinstance(binlog_event, UpdateRowsEvent):\n",
    "                            row_valuesB=list(row['before_values'].values())\n",
    "                            row_valuesA=list(row['after_values'].values())\n",
    "                            \n",
    "                            \n",
    "                            before_data = {columns[i]: datetime_to_str(v) for i, v in enumerate(row_valuesB)}\n",
    "                            after_data = {columns[i]: datetime_to_str(v) for i, v in enumerate(row_valuesA)}\n",
    "\n",
    "                            if after_data:\n",
    "                                message = json.dumps({\n",
    "                                    \"type\": \"UPDATE\",\n",
    "                                    \"database\": schema,\n",
    "                                    \"table\": table,\n",
    "                                    \"before\": before_data,\n",
    "                                    \"after\": after_data\n",
    "                                })\n",
    "                                \n",
    "                                topic = sanitize_topic_name(f\"{schema}.{table}\")\n",
    "                                produce_message(topic, message)\n",
    "\n",
    "\n",
    "                        elif isinstance(binlog_event, DeleteRowsEvent):\n",
    "                            row_valuesD=list(row['values'].values())\n",
    "                            deleted_data = {columns[i]: datetime_to_str(v) for i, v in enumerate(row_valuesD)}\n",
    "\n",
    "                            if deleted_data:\n",
    "                                logger.info(f\"DATABASE: {schema} TABLE: {table}  >>  DELETE: {deleted_data}\")\n",
    "                                message = json.dumps({\n",
    "                                    \"type\": \"DELETE\",\n",
    "                                    \"database\": schema,\n",
    "                                    \"table\": table,\n",
    "                                    \"data\": deleted_data\n",
    "                                })\n",
    "\n",
    "                                topic = sanitize_topic_name(f\"{schema}.{table}\")\n",
    "                                produce_message(topic, message)\n",
    "\n",
    "                        log_pos = binlog_event.packet.log_pos\n",
    "                        save_checkpoint(log_file, log_pos)\n",
    "\n",
    "                elif isinstance(binlog_event, QueryEvent):\n",
    "                    query = binlog_event.query\n",
    "                    schema = binlog_event.schema.decode() if isinstance(binlog_event.schema, bytes) else binlog_event.schema\n",
    "\n",
    "                    if any(keyword in query.upper() for keyword in [\"ALTER TABLE\", \"CREATE TABLE\", \"DROP TABLE\", \"INSERT INTO\", \"UPDATE\", \"DELETE FROM\"]):\n",
    "                        extracted_schema, extracted_table = extract_table_and_schema(query)\n",
    "                        schema = extracted_schema if extracted_schema != 'unknown_schema' else schema\n",
    "\n",
    "                        try:\n",
    "                            create_topic_if_not_exists(schema, extracted_table)\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"Topic creation error: {e}\")\n",
    "                            # Checking Kafka health\n",
    "                            if not check_kafka_health(kafka_cnf):\n",
    "                                logger.critical(\"Kafka health check failed. Exiting...\")\n",
    "                                return\n",
    "\n",
    "                        logger.info(f\"DATABASE: {schema} TABLE: {extracted_table}  >>  QUERY: {query}\")\n",
    "\n",
    "                        message = json.dumps({\n",
    "                            'type': 'QUERY',\n",
    "                            'database': schema,\n",
    "                            'table': extracted_table,\n",
    "                            'query': query\n",
    "                        })\n",
    "\n",
    "                        topic = sanitize_topic_name(f\"{schema}.{extracted_table}\")\n",
    "\n",
    "\n",
    "                        # Checking Kafka health\n",
    "                        if not check_kafka_health(kafka_cnf):\n",
    "                            logger.critical(\"Kafka health check failed. Exiting...\")\n",
    "                            return\n",
    "                        \n",
    "                        produce_message(topic, message)\n",
    "\n",
    "                        log_pos = binlog_event.packet.log_pos\n",
    "                        save_checkpoint(log_file, log_pos)\n",
    "            \n",
    "                error_count=0\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            logger.error(f\"An error occurred: {e}\")\n",
    "\n",
    "            if error_count == 5:  # Increased threshold\n",
    "                detailed_error_message = f\"Error occurred: {e}, Log file: {log_file}, Log position: {log_pos}\"\n",
    "                send_email_alert(\"Error occurred in processing\", detailed_error_message)\n",
    "                return\n",
    "                \n",
    "        finally:\n",
    "            stream.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fea174-d63d-426a-96db-3362a9c21585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a1724-a903-405f-8c95-26a678f6eacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093daf70-c91b-4c55-8cc6-cf1c159829f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
